{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea6_Segmentacion_Semantica.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpEB8dqAK3YsG9pN2/nlaP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abyM3q16qPD5"
      },
      "source": [
        "# Tarea 6: Segmentacion semantica.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SVtdfDTo1tV"
      },
      "source": [
        "import os"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVrT7IBxrLa0"
      },
      "source": [
        "if not os.path.exists('pytorch_unet'):\r\n",
        "  # A continuacion se clona el repo\r\n",
        "  !git clone https://github.com/milesial/Pytorch-UNet.git\r\n",
        "  # Se instalan los requisitos del repo\r\n",
        "  !pip install -r Pytorch-UNet/requirements.txt\r\n",
        "  # se renombra para poder importar las funciones\r\n",
        "  !mv Pytorch-UNet pytorch_unet"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BrPl79EsPNZ"
      },
      "source": [
        "if not os.path.exists('data_semantics.zip'):\r\n",
        "  !wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip\r\n",
        "  !unzip data_semantics.zip"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSqvPDTv03TA"
      },
      "source": [
        "if not os.path.exists('kitti_inverse_map_1channel.py'):\r\n",
        "  !wget https://raw.githubusercontent.com/Diego-II/Procesamiento-Avanzado-de-Imagenes/master/Tarea6/kitti_inverse_map_1channel.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQOFnr8Yqo16"
      },
      "source": [
        "# Importamos las funciones del repositorio\r\n",
        "from pytorch_unet.unet import UNet\r\n",
        "from kitti_inverse_map_1channel import kitti_inverse_map_1channel"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7NuedkCsGJC"
      },
      "source": [
        "from os.path import splitext\r\n",
        "from os import listdir\r\n",
        "import numpy as np\r\n",
        "from glob import glob\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import logging\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "\r\n",
        "class BasicDataset(Dataset):\r\n",
        "  def __init__(self, imgs_dir, masks_dir, read_mask, scale=1, mask_suffix=''):\r\n",
        "  # def __init__(self, imgs_dir, masks_dir, scale=1, mask_suffix=''):\r\n",
        "    self.imgs_dir = imgs_dir\r\n",
        "    self.masks_dir = masks_dir\r\n",
        "    self.read_mask = read_mask\r\n",
        "    self.scale = scale\r\n",
        "    self.mask_suffix = mask_suffix\r\n",
        "    assert 0 < scale <= 1, 'Scale must be between 0 and 1'\r\n",
        "\r\n",
        "    self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\r\n",
        "                if not file.startswith('.')]\r\n",
        "    logging.info(f'Creating dataset with {len(self.ids)} examples')\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.ids)\r\n",
        "\r\n",
        "  @classmethod\r\n",
        "  def preprocess(cls, pil_img, scale):\r\n",
        "    w, h = pil_img.size\r\n",
        "    newW, newH = int(scale * w), int(scale * h)\r\n",
        "    assert newW > 0 and newH > 0, 'Scale is too small'\r\n",
        "    pil_img = pil_img.resize((newW, newH))\r\n",
        "\r\n",
        "    img_nd = np.array(pil_img)\r\n",
        "\r\n",
        "    if len(img_nd.shape) == 2:\r\n",
        "      img_nd = np.expand_dims(img_nd, axis=2)\r\n",
        "\r\n",
        "    # HWC to CHW\r\n",
        "    img_trans = img_nd.transpose((2, 0, 1))\r\n",
        "    if img_trans.max() > 1:\r\n",
        "      img_trans = img_trans / 255\r\n",
        "\r\n",
        "    return img_trans\r\n",
        "\r\n",
        "  def __getitem__(self, i):\r\n",
        "    idx = self.ids[i]\r\n",
        "    mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\r\n",
        "    img_file = glob(self.imgs_dir + idx + '.*')\r\n",
        "\r\n",
        "    assert len(mask_file) == 1, \\\r\n",
        "      f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\r\n",
        "    assert len(img_file) == 1, \\\r\n",
        "      f'Either no image or multiple images found for the ID {idx}: {img_file}'\r\n",
        "    # mask = Image.open(mask_file[0])\r\n",
        "    mask = kitty_inverse_map_1channel(np.array(mask, dtype=np.int32))\r\n",
        "    img = Image.open(img_file[0])\r\n",
        "\r\n",
        "    assert img.size == mask.size, \\\r\n",
        "      f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\r\n",
        "\r\n",
        "    # img = self.preprocess(img, self.scale)\r\n",
        "    # mask = self.preprocess(mask, self.scale)\r\n",
        "\r\n",
        "    return {\r\n",
        "      'image': torch.from_numpy(img).type(torch.FloatTensor),\r\n",
        "      'mask': torch.from_numpy(mask).type(torch.FloatTensor)\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "class CarvanaDataset(BasicDataset):\r\n",
        "  def __init__(self, imgs_dir, masks_dir, scale=1):\r\n",
        "    super().__init__(imgs_dir, masks_dir, scale, mask_suffix='_mask')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ4r3D_iy51l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}